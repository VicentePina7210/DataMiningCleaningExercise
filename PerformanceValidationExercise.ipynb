{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VicentePina7210/DataMiningCleaningExercise/blob/main/PerformanceValidationExercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "UaHfbSV9A7Rk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load dataset\n",
        "path = kagglehub.dataset_download(\"himanshunakrani/iris-dataset\")\n",
        "data_df = pd.read_csv(os.path.join(path, \"iris.csv\"))\n",
        "data_df.head()\n",
        "x = data_df['species'].unique()\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9rWDJ1rBHIl",
        "outputId": "221f8ba2-b955-44f1-87f2-713fd13f0ec6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
            "['setosa' 'versicolor' 'virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement a train test split\n",
        "# (your code here)\n",
        "x_train = data_df.drop(columns = ['species'])\n",
        "y_train = data_df['species']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "kelmldEWCRj2"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the following functions\n",
        "def accuracy(y_true, y_pred):\n",
        "  # (your code here)\n",
        "  # Find the ratio of correct predictions to total predictions\n",
        "  # The sum method counts the amount of true values in this statement\n",
        "  correct = sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred))\n",
        "  return correct / len(y_true)\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    # For multiclass, calculate precision for each class and average\n",
        "    classes = np.unique(y_true)\n",
        "    precisions = []\n",
        "\n",
        "    for cls in classes:\n",
        "        true_positives = sum((y_t == cls and y_p == cls) for y_t, y_p in zip(y_true, y_pred))\n",
        "        predicted_positives = sum(y_p == cls for y_p in y_pred)\n",
        "        precision_cls = true_positives / predicted_positives if predicted_positives != 0 else 0\n",
        "        precisions.append(precision_cls)\n",
        "\n",
        "    return np.mean(precisions)\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    # For multiclass, calculate recall for each class and average\n",
        "    classes = np.unique(y_true)\n",
        "    recalls = []\n",
        "\n",
        "    for cls in classes:\n",
        "        true_positives = sum((y_t == cls and y_p == cls) for y_t, y_p in zip(y_true, y_pred))\n",
        "        actual_positives = sum(y_t == cls for y_t in y_true)\n",
        "        recall_cls = true_positives / actual_positives if actual_positives != 0 else 0\n",
        "        recalls.append(recall_cls)\n",
        "\n",
        "    return np.mean(recalls)\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    # For multiclass, calculate F1 score for each class and average\n",
        "    prec = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    return 2 * (prec * rec) / (prec + rec) if (prec + rec) != 0 else 0\n",
        "\n",
        "def performance_metrics(y_true, y_pred):\n",
        "  # A single function to preint all performance metrics\n",
        "  # (your code here)\n",
        "    print(\"Accuracy:\", accuracy(y_true, y_pred))\n",
        "    print(\"Precision:\", precision(y_true, y_pred))\n",
        "    print(\"Recall:\", recall(y_true, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
        "    return"
      ],
      "metadata": {
        "id": "vT0po9oFDUmc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a logistic regression model and print all performance metrics on the training set\n",
        "# (your code here)\n",
        "# Split the data into training and testing sets\n",
        "X = data_df.drop(columns=['species'])  # Assuming 'species' is the target column\n",
        "y = data_df['species']\n",
        "\n",
        "# Use train_test_split to split data (though only training data will be evaluated here)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the logistic regression model\n",
        "model = LogisticRegression(max_iter=200)  # max_iter is set higher to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_pred_train = model.predict(X_train)\n",
        "\n",
        "# Print all performance metrics for the training set using the functions you defined\n",
        "print(\"Performance Metrics on the Training Set:\")\n",
        "performance_metrics(y_train, y_pred_train)  # Test\n"
      ],
      "metadata": {
        "id": "OJJYTpr1C_uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbddca97-1d4a-4dc2-b262-53c4dd712fc3"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Metrics on the Training Set:\n",
            "Accuracy: 0.975\n",
            "Precision: 0.9761904761904763\n",
            "Recall: 0.975609756097561\n",
            "F1 Score: 0.9759000297530497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement k-fold cross validation\n",
        "x = data_df.drop(columns = ['species'])\n",
        "y = data_df['species']\n",
        "\n",
        "def k_fold_cross_validation(X, y, k):\n",
        "    fold_size = len(X) // k\n",
        "    fold = 1\n",
        "\n",
        "    # Manually splitting data into k folds\n",
        "    for i in range(k):\n",
        "        # Creating validation and training sets for each fold\n",
        "        start = i * fold_size\n",
        "        end = start + fold_size\n",
        "        X_val = X[start:end]\n",
        "        y_val = y[start:end]\n",
        "\n",
        "        # Combine the other folds for training\n",
        "        X_train = pd.concat([X[:start], X[end:]], axis=0)\n",
        "        y_train = pd.concat([y[:start], y[end:]], axis=0)\n",
        "\n",
        "        # Initialize and train the logistic regression model\n",
        "        model = LogisticRegression(max_iter=200)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions on both the training and validation sets\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_val = model.predict(X_val)\n",
        "\n",
        "        # Print performance metrics for both training and validation sets\n",
        "        print(f\"Fold {fold} Performance Metrics:\")\n",
        "\n",
        "        print(\"Training Set Metrics:\")\n",
        "        performance_metrics(y_train, y_pred_train)  # Training set metrics\n",
        "\n",
        "        print(\"Validation Set Metrics:\")\n",
        "        performance_metrics(y_val, y_pred_val)  # Validation set metrics\n",
        "        print() # Line for readability\n",
        "\n",
        "        fold += 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1xrCLf7NECiz"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a logistic regression model and print all performance metrics on the training set AND validation set for each fold\n",
        "k_fold_cross_validation(x, y, k=10)"
      ],
      "metadata": {
        "id": "wzwDdtPMDPu8",
        "outputId": "6a36442d-5863-476d-e9cd-830786c0235d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Performance Metrics:\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.9703703703703703\n",
            "Precision: 0.9738247863247862\n",
            "Recall: 0.9733333333333333\n",
            "F1 Score: 0.9735789978089187\n",
            "Validation Set Metrics:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "\n",
            "Fold 2 Performance Metrics:\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.9703703703703703\n",
            "Precision: 0.9738247863247862\n",
            "Recall: 0.9733333333333333\n",
            "F1 Score: 0.9735789978089187\n",
            "Validation Set Metrics:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "\n",
            "Fold 3 Performance Metrics:\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.9703703703703703\n",
            "Precision: 0.9738247863247862\n",
            "Recall: 0.9733333333333333\n",
            "F1 Score: 0.9735789978089187\n",
            "Validation Set Metrics:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "\n",
            "Fold 4 Performance Metrics:\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.9703703703703703\n",
            "Precision: 0.9719973009446695\n",
            "Recall: 0.9683333333333334\n",
            "F1 Score: 0.9701618577650114\n",
            "Validation Set Metrics:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "\n",
            "Fold 5 Performance Metrics:\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.9851851851851852\n",
            "Precision: 0.9871794871794872\n",
            "Recall: 0.980952380952381\n",
            "F1 Score: 0.9840560828835536\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.9333333333333333\n",
            "Precision: 1.0\n",
            "Recall: 0.9333333333333333\n",
            "F1 Score: 0.9655172413793104\n",
            "\n",
            "Fold 6 Performance Metrics:\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.9925925925925926\n",
            "Precision: 0.9934640522875817\n",
            "Recall: 0.9904761904761905\n",
            "F1 Score: 0.9919678714859438\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.8666666666666667\n",
            "Precision: 1.0\n",
            "Recall: 0.8666666666666667\n",
            "F1 Score: 0.9285714285714286\n",
            "\n",
            "Fold 7 Performance Metrics:\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.9777777777777777\n",
            "Precision: 0.9791666666666666\n",
            "Recall: 0.975\n",
            "F1 Score: 0.9770788912579956\n",
            "Validation Set Metrics:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "\n",
            "Fold 8 Performance Metrics:\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.9629629629629629\n",
            "Precision: 0.9607843137254902\n",
            "Recall: 0.958095238095238\n",
            "F1 Score: 0.9594378917046207\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.8666666666666667\n",
            "Precision: 1.0\n",
            "Recall: 0.8666666666666667\n",
            "F1 Score: 0.9285714285714286\n",
            "\n",
            "Fold 9 Performance Metrics:\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.9703703703703703\n",
            "Precision: 0.9706682206682208\n",
            "Recall: 0.9647619047619047\n",
            "F1 Score: 0.9677060506181838\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.8666666666666667\n",
            "Precision: 1.0\n",
            "Recall: 0.8666666666666667\n",
            "F1 Score: 0.9285714285714286\n",
            "\n",
            "Fold 10 Performance Metrics:\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.9703703703703703\n",
            "Precision: 0.9706682206682208\n",
            "Recall: 0.9647619047619047\n",
            "F1 Score: 0.9677060506181838\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.9333333333333333\n",
            "Precision: 1.0\n",
            "Recall: 0.9333333333333333\n",
            "F1 Score: 0.9655172413793104\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different values of K. How do the performance metrics vary based on different values of K?\n",
        "\n",
        "# How do you interpret the observations from the previous question\n"
      ],
      "metadata": {
        "id": "0kIlPVc7EP0u",
        "outputId": "02fd9a1a-15d0-45e9-c3af-5c2315a22b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Class Distribution:\n",
            "species\n",
            "versicolor    41\n",
            "setosa        40\n",
            "virginica     39\n",
            "Name: count, dtype: int64\n",
            "Validation Set Class Distribution:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_val' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-c267c6891bc2>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Set Class Distribution:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_val' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At k = 3 the model does nto perform very well and because the validation accuracy precision, recall and f1 score just get set to 0. This could possibly be because there is not enough data to make an accurate prediction\n",
        "\n",
        "at k = 5 we do not see much variance between the accuracy and the precision as well as in the validation which leads me to believe the model is performing very well and may be the right size for this dataset.\n",
        "\n",
        "k = 10 does not show much variance either, however the output of many of the metrics showing 100% which makes me skeptical of the accuracy, this could be because the dataset is overfitting and not learning the general patterns that would apply to new data\n",
        "\n"
      ],
      "metadata": {
        "id": "M0MF-6XIBjh7"
      }
    }
  ]
}