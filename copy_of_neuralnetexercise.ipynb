{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VicentePina7210/DataMiningCleaningExercise/blob/main/copy_of_neuralnetexercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Implementation and Experimentation\n",
        "\n",
        "Instructions:\n",
        "1. This script loads a dataset using Pandas and implements a minimal neural network in PyTorch.\n",
        "2. The default architecture is basic, but you will modify it to experiment with different choices.\n",
        "3. Your goal is to observe the effects of architecture, activation functions, and optimization strategies on training.\n",
        "4. Answer the experimental and deep-dive questions at the end based on your findings."
      ],
      "metadata": {
        "id": "Toluh7mBIDol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "ZCRPjCVpIKgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================== 1. Load Data ========================\n",
        "\n",
        "# Load any dataset of choice (ensure it's a regression dataset)\n",
        "df = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n",
        "df[\"median_house_value\"] = df[\"median_house_value\"] / 100_000\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "hvGIPdBKINSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume last column is target, rest are features\n",
        "x = df[[\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]].values\n",
        "y = df[\"median_house_value\"].values\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x = torch.tensor(x, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # Ensure y is 2D\n",
        "\n",
        "# Split data\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "VSUZnvVwIljC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVyGg0UlH9qu"
      },
      "outputs": [],
      "source": [
        "# ======================== 2. Define Minimal Neural Network ========================\n",
        "class CustomNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(CustomNN, self).__init__()\n",
        "        self.hidden_0 = nn.Linear(input_dim, 150)\n",
        "        self.hidden_1 = nn.Linear(150,150)\n",
        "        self.hidden_2 = nn.Linear(150,150)\n",
        "        self.output = nn.Linear(150, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden_0(x))\n",
        "        x = torch.relu(self.hidden_1(x))\n",
        "        x = torch.relu(self.hidden_2(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# Initialize Model\n",
        "input_dim = x.shape[1]\n",
        "model = CustomNN(input_dim)\n",
        "\n",
        "# Loss Function & Optimizer\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================== 3. Training Loop ========================\n",
        "epochs = 1500\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward Pass\n",
        "    y_pred = model.forward(x_train)\n",
        "    loss = loss_function(y_pred, y_train)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store loss for plotting\n",
        "    train_losses.append(loss.item())\n",
        "    with torch.no_grad():\n",
        "        y_pred_valid = model.forward(x_valid)\n",
        "        valid_loss = loss_function(y_pred_valid, y_valid)\n",
        "        valid_losses.append(valid_loss.item())\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Plot Loss Curve\n",
        "plt.plot(train_losses)\n",
        "plt.plot(valid_losses)\n",
        "plt.legend([\"Training Loss\", \"Validation Loss\"])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vkczFsbAJvJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================== 4. Evaluation ========================\n",
        "with torch.no_grad():\n",
        "    y_pred_test = model(x_valid)\n",
        "    test_loss = loss_function(y_pred_test, y_valid)\n",
        "    print(f\"Final Valid Loss: {test_loss.item():.4f}\")\n",
        "\n",
        "# Print some predicted vs actual prices\n",
        "print(\"===== Example outputs =====\")\n",
        "for i in range(10):\n",
        "    print(f\"Predicted: ${100_000 * y_pred_test[i].item():.2f}, Actual:${100_000 * y_valid[i].item():.2f}\")"
      ],
      "metadata": {
        "id": "KuzYPaw3J0cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1: Experimentation **\n",
        "1. Change the number of neurons in the hidden layer. What is the effect and what is the ideal number?\n",
        "\n",
        "It seems like the higher the number of neurons in the network the better it performs, maybe because it is able to pick up on patterns better. It also minimizes the loss, at 1000 neurons it does not perform well, 100 seems to be better\n",
        "\n",
        "2. Add a few extra layers in the neural network. How does this affect performance and what is a good number?\n",
        "adding more layers into the network made the models plot look like it was more accurate, but the overall loss was still higher than what would be ideal. I found that 2 layers work nicely\n",
        "\n",
        "\n",
        "3. Play around with the learning rate, how does this affect learning. What is an ideal number?\n",
        "\n",
        "Making the learning rate too high makes the model take large steps in either direction when it comes to the error graph, then the model predicts the same thing for many inputs, it is likely it preents it from picking up a pattern accurately perhaps it is taking too many large of steps.\n",
        "Making the learning rate too small, made\n",
        "\n",
        "4. Play around with the number of epochs, how does this affect learning. What is an ideal number?\n",
        "\n",
        "It seems that it is nice to have a larger number of epochs, it allows the model to make more iterations and learn the data better. However I noticed that adding too many epochs can cause the model to overfit.\n",
        "\n",
        "*** EXTRA CREDIT***\n",
        "After some trial and error and logging the results it seems that for the best model with the best looking curve, 150 neurons, 2 layers, lr =.001 and epoch of 1500 was the best with a high .4 valid loss. After the layers increased too much or the epochs went over 1500 the lowest valid loss I got was in the high .3.\n",
        "\n",
        "To avoid overfitting I left the code as is with 250 neurons, 2 layers and 1500 epochs with a .001 learning rate. This resulted in a nice curve and a model that generalized well without overfitting. Valid loss = 4.627\n",
        "******************\n",
        "\n",
        "\n",
        "5. Try using Tanh, Sigmoid, or LeakyReLU instead of ReLU. How does the activation function affect learning?\n",
        "I think this question is worded wrong because we are already using Sigmoid\n",
        "\n",
        "With ReLu, the functions learning curve was a lot steeper and once it got to a spot where not much more adjustment was needed, it stayed flat. The initial loss was also much higher at the start, and ReLU was also much faster than sigmoid and tanh\n",
        "\n",
        "With tanh the learning curve was much smoother, but the final valid loss was higher than both sigmoid and ReLU. It was also much slower, it does not seem useful for this case. Maybe it would be useful for some scenarios that require minor fine tuned adjustment\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. Try removing all activation functions. How does this affect learning? What is happening mathematically?\n",
        "Without the activation functions, there is no longer a threshold for neurons to be activated. The model ends up becoming linear, adding layers does not help. Overall the model becomes simplified.\n",
        "\n",
        "\n",
        "7. What is dropout in neural networks? Use dropout after the hidden layer. How does it affect performance?\n",
        "\n",
        "\n",
        "\n",
        "8. How can modifying the scale of the input values help learning?\n",
        "\n",
        "\n",
        "\n",
        "9. Plot the predicted values vs. actual values. Are there any patterns you notice? What are ways this can be addressed?\n",
        "\n"
      ],
      "metadata": {
        "id": "SBvMaEC-KX0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Deeper Questions**\n",
        "1. Why do activation functions matter? Why wouldn't we use an activation function on the output layer for regression?\n",
        "\n",
        "Activation functions keep the outputs within a realistic range, for continous values, sometimes the range is unpredictable or should not have much of a bound. For example the price of houses in the housing market. The price should not be set to a bound between 0 and 1\n",
        "\n",
        "2. What’s the tradeoff between using more layers vs. a single-layer model?\n",
        "\n",
        "The more layers there are in a model, the better its ability to learn more complex patterns, such as deep learning  and image analyzation.\n",
        "Single layer models are good for smaller datasets and for a quicker compilation if resources are in question.\n",
        "\n",
        "\n",
        "3. How does the choice of optimizer affect convergence? Why do some optimizers work better than others?\n",
        "\n",
        "The optimizer you choose, can influence how fast and how well the model learns. Some optimizers have built in variable learning rates while others such as the one we are using, have a fixed learning rate. The ones with a built in learning rate typically move towards the local minimum faster.\n",
        "\n",
        "\n",
        "4. If you see overfitting, what changes can you make to the model?\n",
        "\n",
        "To combat overfitting, the first thing in most cases would be to add more data if available. In this case its not an option, so for this model we can use dropout, to set some neurons to 0, this can make the model learn more general patterns instead of overfitting.\n",
        "\n",
        "\n",
        "5. In a real-world scenario, how would you determine if a neural network is the right model for a regression problem compared to, say, a Random Forest?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s8Ll65m-MpGA"
      }
    }
  ]
}