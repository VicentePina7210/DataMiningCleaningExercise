{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VicentePina7210/DataMiningCleaningExercise/blob/main/Copy_of_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Implementation\n",
        "Modify the code and answer the questions below.\n"
      ],
      "metadata": {
        "id": "1vGYEF4kN4if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "kF1pQVhDXIkl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_v7pcKr5VjHN"
      },
      "outputs": [],
      "source": [
        "# ==================== 1. Load and Preprocess Data ====================\n",
        "# Read text from file\n",
        "with open(\"story.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create character mappings\n",
        "chars = sorted(set(text))  # Define vocabulary\n",
        "\n",
        "# Tokenize\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Convert text to indices - Tokenization\n",
        "data = [char_to_idx[c] for c in text]\n",
        "\n",
        "# Define sequence length\n",
        "seq_length = 20\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 2. Define Dataset Loader ====================\n",
        "def get_batches(data, batch_size, seq_length):\n",
        "    \"\"\"Yield batches of character sequences.\"\"\"\n",
        "    num_batches = len(data) // (batch_size * seq_length)\n",
        "    data = data[:num_batches * batch_size * seq_length]\n",
        "    data = np.reshape(data, (batch_size, -1))\n",
        "\n",
        "    while True:\n",
        "        # Loop over and creating batch\n",
        "        for n in range(0, data.shape[1] - seq_length, seq_length):\n",
        "            x = data[:, n:n+seq_length]\n",
        "            y = data[:, n+1:n+seq_length+1]\n",
        "            yield torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "batch_generator = get_batches(data, 5, seq_length)"
      ],
      "metadata": {
        "id": "lUzzITRTXMI-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 3. Define Custom RNN Model ====================\n",
        "class CustomRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(CustomRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Learnable weight matrices\n",
        "        self.Wxh = nn.Parameter(torch.randn(hidden_size, vocab_size) * 0.01)  # Input -> Hidden\n",
        "        self.Whh = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)  # Hidden -> Hidden\n",
        "        self.Why = nn.Parameter(torch.randn(vocab_size, hidden_size) * 0.01)  # Hidden -> Output\n",
        "\n",
        "        # Bias terms\n",
        "        self.bh = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.by = nn.Parameter(torch.zeros(vocab_size))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Compute the forward pass manually.\"\"\"\n",
        "        batch_size, seq_length = x.shape\n",
        "\n",
        "        # One hot encode input sequence of tokens\n",
        "        one_hot = torch.zeros(batch_size, seq_length, vocab_size)\n",
        "        one_hot.scatter_(2, x.unsqueeze(-1), 1)  # Convert to one-hot\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            xt = one_hot[:, t, :]  # Input at time step t\n",
        "            hidden = torch.tanh(torch.mm(xt, self.Wxh.T) + torch.mm(hidden, self.Whh.T) + self.bh)  # Compute hidden state\n",
        "            yt = torch.mm(hidden, self.Why.T) + self.by  # Compute output\n",
        "            outputs.append(yt)\n",
        "\n",
        "        return torch.stack(outputs, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(batch_size, self.hidden_size)  # Initialize hidden state\n",
        "\n",
        "# Test\n",
        "# model = CustomRNN(vocab_size, 80)\n",
        "# y, h = model.forward(torch.tensor([[1, 2, 3]]), torch.zeros(1, 80))"
      ],
      "metadata": {
        "id": "SKGWeuoBXPHv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "hidden_size = 80\n",
        "batch_size = 5\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Initialize model\n",
        "model = CustomRNN(vocab_size, hidden_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "nyxFSw7Ot12l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 4. Train the Model ====================\n",
        "num_epochs = 10\n",
        "max_steps = 10000 # TODO implement\n",
        "train_loader = get_batches(data, batch_size, seq_length)\n",
        "for epoch in range(num_epochs):\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(x, hidden.detach())\n",
        "        loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Step {i}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XyQadeIpXSRB",
        "outputId": "9d611a24-2910-4edf-faef-26316db8434d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Step 0, Loss: 1.9775\n",
            "Epoch 1/10, Step 1000, Loss: 2.0324\n",
            "Epoch 1/10, Step 2000, Loss: 1.8433\n",
            "Epoch 1/10, Step 3000, Loss: 1.7854\n",
            "Epoch 1/10, Step 4000, Loss: 2.2297\n",
            "Epoch 1/10, Step 5000, Loss: 2.0295\n",
            "Epoch 1/10, Step 6000, Loss: 1.9557\n",
            "Epoch 1/10, Step 7000, Loss: 1.8873\n",
            "Epoch 1/10, Step 8000, Loss: 2.0260\n",
            "Epoch 1/10, Step 9000, Loss: 2.1504\n",
            "Epoch 1/10, Step 10000, Loss: 2.3154\n",
            "Epoch 1/10, Step 11000, Loss: 2.0448\n",
            "Epoch 1/10, Step 12000, Loss: 1.9994\n",
            "Epoch 1/10, Step 13000, Loss: 2.1740\n",
            "Epoch 1/10, Step 14000, Loss: 2.1441\n",
            "Epoch 1/10, Step 15000, Loss: 2.2635\n",
            "Epoch 1/10, Step 16000, Loss: 1.9761\n",
            "Epoch 1/10, Step 17000, Loss: 1.9625\n",
            "Epoch 1/10, Step 18000, Loss: 2.1826\n",
            "Epoch 1/10, Step 19000, Loss: 1.7389\n",
            "Epoch 1/10, Step 20000, Loss: 2.1678\n",
            "Epoch 1/10, Step 21000, Loss: 1.8271\n",
            "Epoch 1/10, Step 22000, Loss: 2.0678\n",
            "Epoch 1/10, Step 23000, Loss: 2.0897\n",
            "Epoch 1/10, Step 24000, Loss: 1.9310\n",
            "Epoch 1/10, Step 25000, Loss: 1.8829\n",
            "Epoch 1/10, Step 26000, Loss: 2.0707\n",
            "Epoch 1/10, Step 27000, Loss: 1.8584\n",
            "Epoch 1/10, Step 28000, Loss: 1.8839\n",
            "Epoch 1/10, Step 29000, Loss: 1.7996\n",
            "Epoch 1/10, Step 30000, Loss: 2.0367\n",
            "Epoch 1/10, Step 31000, Loss: 2.1807\n",
            "Epoch 1/10, Step 32000, Loss: 2.1473\n",
            "Epoch 1/10, Step 33000, Loss: 2.0797\n",
            "Epoch 1/10, Step 34000, Loss: 2.4042\n",
            "Epoch 1/10, Step 35000, Loss: 2.1172\n",
            "Epoch 1/10, Step 36000, Loss: 1.7486\n",
            "Epoch 1/10, Step 37000, Loss: 2.0664\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-678b5d7eafc1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 5. Generate New Text ====================\n",
        "def generate_text(model, start_str=\"Once upon a time\", length=500):\n",
        "    \"\"\"Generate text using the trained model.\"\"\"\n",
        "    model.eval()\n",
        "    chars = [char_to_idx[c] for c in start_str]\n",
        "    input_seq = torch.tensor(chars, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    hidden = model.init_hidden(1)\n",
        "    generated_text = start_str\n",
        "\n",
        "    for _ in range(length):\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "            probs = torch.softmax(output[:, -1, :], dim=1).squeeze()\n",
        "            next_idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "        generated_text += idx_to_char[next_idx]\n",
        "        input_seq = torch.tensor([[next_idx]], dtype=torch.long)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate a sample text\n",
        "print(generate_text(model, start_str=\"Once upon a time\", length=300))\n"
      ],
      "metadata": {
        "id": "_S5akqwqXT-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ae2b95-e2c8-43f1-bc32-7a5eff4c92c7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time foted nenesepeverald wet? Wellteo and the wh foosein iantt and poald presere of his to ktofery nerald to ank she letals,t. At to aprit -oghed the alint bot thouma Selaer so of heren drees,t, pet rar his ore deted to ond of woo loon the ios, tamatore thestos,.., meser even of and gnellyer ablinse..,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions / tasks\n",
        "(The training for this model takes time so you may terminate it after a set number of steps)\n",
        "\n",
        "\n",
        "1. What do you observe as the model outputs as training progresses?\n",
        "\n",
        "at each step, the model makes adjustments to reduce loss, sometimes it fluctuates and will increase loss but over the general trend it reduces the loss\n",
        "\n",
        "2. What is the minimum hidden size that still allows the model to learn good outputs?\n",
        "\n",
        "Letting the model train for up to 20 min with 2 layers still did not get me a good result, the semantics seemed to start to be closer to making sense but it was not quite there yet. I think the network would be better trained with words instead of each character.    \n",
        "\n",
        "\n",
        "3. What do you think the effect of changing the sequence length is? Try different values and describe your observations\n",
        "\n",
        "I believe that changing the sequence length to be higher may cause the training process to be slower but it will improve the final valid loss.\n",
        "\n",
        "First test was with a short sequence length, the model only sees a small window of context each time. It moved through epochs faster but the end result ended up being the model producing complete nonsense.\n",
        "\n",
        "With a long sequence length the model makes more sense but it takes longer to train\n",
        "\n",
        "5. Modify the code to add another recurrent layer to the network (this may take some research). How do you think this will affect the model? What do you actually observe?\n",
        "The training time increasedd by a lot, but it also made the model produce a much higher quality output. However I think this also increases the liklihood of the model overfitting.\n",
        "\n",
        "\n",
        "4. In the case of a generative language model, how can we tell if the model has actually \"understood\" language as opposed to just memorizing common patterns\n",
        "\n",
        "The model would be able to generate coherent sentences and responses, it would develop true reasoning and understanding instead of just being able to string together high quality sentences based on patterns.\n",
        "\n",
        "\n",
        "5. What are the pros and cons of using a word-level vocabulary as opposed to a character-level vocabulary?\n",
        "\n",
        "Word level vocabulary allows the model to have an output that sounds like it makes some sort of sense, it is much easier to learn semantics of language when training through words than it is from character. The cons are that the training data would be smaller because there are many letters in a single word, the training would also take much longer."
      ],
      "metadata": {
        "id": "AgwZtlDGZw_t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zqd3AcZvW1MZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}